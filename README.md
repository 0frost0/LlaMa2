# LLaMA2-Tiny Implementation from Scratch

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-Apache%202.0-green)](./LICENSE)

æœ¬é¡¹ç›®æ˜¯åŸºäº [Datawhale Happy-LLM (ä»é›¶å¼€å§‹çš„å¤§æ¨¡å‹)](https://github.com/datawhalechina/happy-llm) æ•™ç¨‹ç¬¬äº”ç« å†…å®¹çš„å¤ç°ä¸å®ç°ã€‚

é¡¹ç›®æ—¨åœ¨**ä»é›¶å¼€å§‹ï¼ˆFrom Scratchï¼‰**æ„å»ºå¹¶è®­ç»ƒä¸€ä¸ªåŸºäº LLaMA2 æ¶æ„çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆTiny-LLMï¼‰ã€‚é¡¹ç›®åŒ…å«å®Œæ•´çš„æ¨¡å‹ç»“æ„å®ç°ï¼ˆRMSNorm, RoPE, GQA, SwiGLUï¼‰ã€Tokenizer è®­ç»ƒã€é¢„è®­ç»ƒï¼ˆPretrainï¼‰ä»¥åŠæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„å…¨æµç¨‹ä»£ç ã€‚

## ğŸ“– é¡¹ç›®ç‰¹è‰²

* **æ¨¡å‹æ¶æ„**ï¼šå®Œå…¨æ‰‹å†™å®ç° LLaMA2 æ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬ï¼š
    * **RMSNorm** (Root Mean Square Layer Normalization)
    * **RoPE** (Rotary Positional Embeddings) æ—‹è½¬ä½ç½®ç¼–ç 
    * **GQA** (Grouped-Query Attention) åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶
    * **SwiGLU** æ¿€æ´»å‡½æ•°çš„å‰é¦ˆç¥ç»ç½‘ç»œ
* **å…¨æµç¨‹è®­ç»ƒ**ï¼šåŒ…å«ä»æ•°æ®æ¸…æ´—ã€Tokenizer è®­ç»ƒã€æ¨¡å‹é¢„è®­ç»ƒåˆ°æŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰çš„å®Œæ•´æµæ°´çº¿ã€‚
* **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šæ”¯æŒä½¿ç”¨ **DeepSpeed** è¿›è¡Œå¤šå¡åˆ†å¸ƒå¼è®­ç»ƒã€‚
* **å®éªŒè¿½è¸ª**ï¼šé›†æˆ **SwanLab** è¿›è¡Œè®­ç»ƒè¿‡ç¨‹çš„å¯è§†åŒ–ç›‘æ§ã€‚

## âš™ï¸ æ¨¡å‹é…ç½® (Tiny-LLM)

æœ¬é¡¹ç›®é»˜è®¤è®­ç»ƒçš„æ¨¡å‹é…ç½®å¦‚ä¸‹ï¼ˆçº¦ 215M å‚æ•°ï¼‰ï¼š

| å‚æ•° | å€¼ | è¯´æ˜ |
| :--- | :--- | :--- |
| `dim` | 1024 | éšè—å±‚ç»´åº¦ |
| `n_layers` | 18 | Transformer å±‚æ•° |
| `n_heads` | 16 | æ³¨æ„åŠ›å¤´æ•° |
| `n_kv_heads` | 8 | KV å¤´æ•° (GQA) |
| `vocab_size` | 6144 | è¯è¡¨å¤§å° |
| `max_seq_len` | 512 | æœ€å¤§åºåˆ—é•¿åº¦ |

## ğŸ› ï¸ ç¯å¢ƒå®‰è£…

1. å…‹éš†ä»“åº“ï¼š
   ```bash
   git clone [https://github.com/0frost0/LlaMa2.git](https://github.com/0frost0/LlaMa2.git)
   cd LlaMa2
